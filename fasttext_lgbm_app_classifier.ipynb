{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cc1f07",
   "metadata": {},
   "source": [
    "# App Type Classifier with fastText embeddings + LightGBM\n",
    "\n",
    "This notebook builds a multi-class model to detect whether a process/app is a **Game**, **Browser**, or another category using the last column `label`.\n",
    "\n",
    "**Pipeline overview**\n",
    "1. Load the CSV (strings + numerics).\n",
    "2. Concatenate all **string columns** into a single text field.\n",
    "3. Train **fastText** (unsupervised) on the corpus and obtain a dense sentence embedding per row.\n",
    "4. Combine the fastText embedding with the numeric features.\n",
    "5. Train a **LightGBM** classifier and evaluate with F1/precision/recall and a confusion matrix.\n",
    "6. Save the fastText model, LightGBM model, and preprocessing artifacts.\n",
    "\n",
    "> **Note:** If `fasttext` installation fails on your platform, switch to the fallback TF-IDF baseline cell near the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python --version\n",
    "# Install dependencies (run once per environment)\n",
    "%pip -q install lightgbm scikit-learn pandas matplotlib seaborn joblib\n",
    "# For windows fasttext_win for Linux fasttext\n",
    "%pip install fasttext_win\n",
    "\n",
    "# In a notebook cell or terminal\n",
    "#%pip install fasttext-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math, gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596cb9e",
   "metadata": {},
   "source": [
    "## 1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9849142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV (update if you renamed/moved it)\n",
    "CSV_PATH = '27112025/Data.csv'\n",
    "\n",
    "# Robust CSV load: allow long fields, mixed quoting, and bad lines skipped (log count).\n",
    "kwargs = dict(engine='python')\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH, on_bad_lines='skip', **kwargs)\n",
    "except TypeError:\n",
    "    # pandas<1.3 doesn't support on_bad_lines; use error_bad_lines=False\n",
    "    df = pd.read_csv(CSV_PATH, error_bad_lines=False, **kwargs)\n",
    "\n",
    "print(df.shape)\n",
    "df = df.drop('PID', axis=1)\n",
    "df.head(3)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bcabe2",
   "metadata": {},
   "source": [
    "## 2) Column typing: text vs numeric\n",
    "We will treat `object` dtype columns (except the label) as text. Numeric-like strings are coerced to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "target_col = \"label\"\n",
    "assert target_col in df.columns, \"Expected a column named 'label'\"\n",
    "\n",
    "# Identify text vs numeric columns by dtype\n",
    "all_cols = df.columns.tolist()\n",
    "text_cols = [c for c in all_cols if (df[c].dtype=='object') and (c != target_col)]\n",
    "# Attempt to coerce all non-text, non-target columns to numeric\n",
    "num_candidate_cols = [c for c in all_cols if c not in text_cols + [target_col]]\n",
    "for c in num_candidate_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "numeric_cols = [c for c in num_candidate_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "print('Text columns ({}):'.format(len(text_cols)), text_cols[:10], '...')\n",
    "print('Numeric columns ({}):'.format(len(numeric_cols)), numeric_cols[:10], '...')\n",
    "print('Target distribution:')\n",
    "print(df[target_col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea64f9",
   "metadata": {},
   "source": [
    "## 3) Build a single text field and train fastText\n",
    "We concatenate all string columns into one field and train a **fastText unsupervised** model (`skipgram`).\n",
    "Then, for each row, we obtain a dense sentence vector (default `dim=100`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71308855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate text columns\n",
    "def _norm_text(x):\n",
    "    if pd.isna(x):\n",
    "        return ''\n",
    "    s = str(x)\n",
    "    #s = s.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    #s = re.sub(r'[^A-Za-z0-9_:\\-]+', ' ', s)\n",
    "    return s.lower()\n",
    "\n",
    "df['__text__'] = df[text_cols].astype(str).apply(lambda r: ' '.join(_norm_text(v) for v in r.values), axis=1)\n",
    "\n",
    "# Write corpus for fastText\n",
    "corpus_path = 'corpus.txt'\n",
    "with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "    for line in df['__text__'].tolist():\n",
    "        f.write(line.strip() + '\\n')\n",
    "\n",
    "%pip install fasttext-wheel\n",
    "\n",
    "import fasttext\n",
    "ft_dim = 128\n",
    "ft_model = fasttext.train_unsupervised(corpus_path, model='skipgram', dim=ft_dim, epoch=10, minn=2, maxn=5, lr=0.05, thread=4)\n",
    "#min\n",
    "ft_model.save_model('fasttext_model.bin')\n",
    "print('fastText trained. dim=', ft_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentence embeddings for each row\n",
    "def sent_vec(s: str):\n",
    "    return ft_model.get_sentence_vector(s)\n",
    "\n",
    "emb = np.vstack([sent_vec(s) for s in df['__text__'].values])\n",
    "emb_df = pd.DataFrame(emb, columns=[f'ft_{i}' for i in range(emb.shape[1])], index=df.index)\n",
    "print(emb_df.shape)\n",
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf52a2",
   "metadata": {},
   "source": [
    "## 4) Assemble features + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare numeric matrix\n",
    "X_num = df[numeric_cols].copy() if numeric_cols else pd.DataFrame(index=df.index)\n",
    "# Fill missing numeric values with 0 (LightGBM can handle NaN too; choose per preference)\n",
    "X_num = X_num.fillna(0)\n",
    "\n",
    "# Concatenate numeric + embedding features\n",
    "X = pd.concat([X_num, emb_df], axis=1)\n",
    "y_raw = df[target_col].astype(str).values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "classes = list(le.classes_)\n",
    "print('Classes:', classes)\n",
    "\n",
    "# Train/val split\n",
    "# Handle case where a class has only one sample\n",
    "if np.min(np.bincount(y)) < 2:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    "    )\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36854616",
   "metadata": {},
   "source": [
    "## 5) Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=len(classes),\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=0.1,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    eval_metric='multi_logloss',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    ")\n",
    "\n",
    "print('Best iteration:', lgbm.best_iteration_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef055ce",
   "metadata": {},
   "source": [
    "## 6) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_pred = lgbm.predict(X_test)\n",
    "\n",
    "print(sorted(set(y_test)))\n",
    "\n",
    "print(classification_report(y_test, y_pred, labels=np.arange(len(classes)), target_names=classes, digits=4, zero_division=0))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print('Macro F1:', round(macro_f1, 4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display misclassified data\n",
    "misclassified_mask = y_pred != y_test\n",
    "misclassified_indices = X_test.index[misclassified_mask]\n",
    "misclassified_data = df.loc[misclassified_indices].copy()\n",
    "misclassified_data['predicted_label'] = le.inverse_transform(y_pred[misclassified_mask])\n",
    "print('Misclassified Data:')\n",
    "misclassified_data[['label', 'predicted_label'] + text_cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a26f37",
   "metadata": {},
   "source": [
    "## 7) Feature importance (numeric features only)\n",
    "Embeddings are many anonymous dimensions; to interpret, we highlight the top original numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'gain': lgbm.booster_.feature_importance(importance_type='gain')\n",
    "}).sort_values('gain', ascending=False)\n",
    "top_num = feat_imp[~feat_imp['feature'].str.startswith('ft_')].head(20)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.barplot(data=top_num, y='feature', x='gain', orient='h', palette='viridis')\n",
    "plt.title('Top numeric features by gain')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "top_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd35e1b",
   "metadata": {},
   "source": [
    "## 8) Save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc797ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = Path('artifacts')\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save fastText model already saved as fasttext_model.bin\n",
    "# Save LightGBM model\n",
    "# joblib.dump(lgbm, artifacts_dir / 'lgbm_model.joblib')\n",
    "lgbm.booster_.save_model(artifacts_dir / 'lgbm_model.txt') # Save LightGBM model in a C++ compatible format\n",
    "# Save label encoder and column lists\n",
    "meta = {\n",
    "    'classes': classes,\n",
    "    'text_cols': text_cols,\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'embedding_dim': emb_df.shape[1]\n",
    "}\n",
    "json.dump(meta, open(artifacts_dir / 'meta.json', 'w'))\n",
    "print('Saved to', artifacts_dir.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ce58c",
   "metadata": {},
   "source": [
    "## 9) Inference helper\n",
    "Provide a function that takes a row (dict or pandas.Series) and returns the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ec4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(row: dict):\n",
    "    if not isinstance(row, dict):\n",
    "        row = row.to_dict()\n",
    "    # Build text\n",
    "    text = ' '.join(_norm_text(row.get(c, '')) for c in text_cols)\n",
    "    vec = ft_model.get_sentence_vector(text).reshape(1, -1)\n",
    "    # Numeric\n",
    "    xnum = []\n",
    "    for c in numeric_cols:\n",
    "        v = row.get(c, np.nan)\n",
    "        try:\n",
    "            v = float(v)\n",
    "        except Exception:\n",
    "            v = np.nan\n",
    "        xnum.append(0 if (v is None or (isinstance(v, float) and math.isnan(v))) else v)\n",
    "    xnum = np.array(xnum, dtype=float).reshape(1, -1) if len(xnum)>0 else np.zeros((1,0))\n",
    "    X_infer = np.hstack([xnum, vec]) if xnum.shape[1]>0 else vec\n",
    "    pred = lgbm.predict(X_infer)[0]\n",
    "    return le.inverse_transform([pred])[0]\n",
    "\n",
    "# Quick sanity check on a random held-out sample\n",
    "sample_idx = X_test.sample(1, random_state=SEED).index[0]\n",
    "pred_label = predict_row(df.loc[sample_idx])\n",
    "true_label = le.inverse_transform([y_test[list(X_test.index).index(sample_idx)]])[0]\n",
    "print('Pred vs True:', pred_label, ' / ', true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b39c12",
   "metadata": {},
   "source": [
    "## (Optional) Baseline without fastText: TFâ€‘IDF + LightGBM\n",
    "If fastText is not available on your machine, try this quick baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.9, ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(df['__text__'])\n",
    "Xb = X_tfidf\n",
    "yb = y\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.2, random_state=SEED, stratify=yb)\n",
    "lgbm_b = lgb.LGBMClassifier(objective='multiclass', num_class=len(classes), n_estimators=500, learning_rate=0.1, random_state=SEED)\n",
    "lgbm_b.fit(Xb_train, yb_train)\n",
    "pred_b = lgbm_b.predict(Xb_test)\n",
    "print(classification_report(yb_test, pred_b, target_names=classes, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
